# LLaMA 2 from scratch
*(Work in progress)*

LLaMA 2 model implemented from the beginning.

The purpose of this code is to self-educate, understand and explore the principles of the LLaMA 2 model.

Based on the excellent tutorials by Umar Jamil:
* [LLaMA explained: KV-Cache, Rotary Positional Embedding, RMS Norm, Grouped Query Attention, SwiGLU](https://youtu.be/Mn_9W1nCFLo?si=9A0K9djlJGSn3Rt3), 
* [Coding LLaMA 2 from scratch in PyTorch - KV Cache, Grouped Query Attention, Rotary PE, RMSNorm](https://youtu.be/oM4VmoabDAI?si=fzfzZvq9A9mq3bfn). 

Thank you very much for the excellent tutorial and all credit goes to @hkproj.